{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <div class=\"alert\" align=\"CENTER\" > <style> h1 {color:#3498DB;background:#BFC9CA} </style> <strong> TRAFFIC LIGHT CONTROL WITH DEEP Q-NETWORKS <BR> </strong></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" align=\"CENTER\" >  <strong> GROUP 2 <BR><br> </strong>\n",
    "\n",
    "`Group Members:` <br>Meenu Ramesh<br> Aardran Premakumar <br>Mohammed Nazim</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> IMPORTING THE LIBRARIES</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import traci\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from collections import deque\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> FILTERING THE WARNINGS FROM OUTPUT</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing warnings and configure Pandas to display all columns\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> SETTING UP SUMO ENVIRONMENT</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the environment variable for SUMO and verifying its presence\n",
    "os.environ[\"SUMO_HOME\"] = r'C:\\Program Files (x86)\\Eclipse\\Sumo'\n",
    "\n",
    "if 'SUMO_HOME' not in os.environ:\n",
    "    sys.exit(\"Please declare the 'SUMO_HOME' environment variable.\")\n",
    "\n",
    "# Adding SUMO tools to Python path\n",
    "tools = os.path.join(os.environ['SUMO_HOME'], 'tools')\n",
    "sys.path.append(tools)\n",
    "\n",
    "# Path to the SUMO GUI binary for visual simulation\n",
    "sumoBinary = os.path.join(os.environ['SUMO_HOME'], 'bin', 'sumo-gui.exe')\n",
    "\n",
    "# Configuration file path and command setup for starting SUMO with TraCI\n",
    "sumoConfigPath = '.\\location' # location of the SUMO configuration file\n",
    "sumoCmd = [sumoBinary, \"-c\", sumoConfigPath, \"--start\", \"--quit-on-end\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> DQN MODEL ARCHITECTURE</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network architecture for the DQN\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10, activation='relu'))  # Input layer with state size(10)\n",
    "model.add(Dense(32, activation='relu')) # Hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(4, activation='linear'))  # Output layer with action size(4)\n",
    "model.compile(loss='mse', optimizer=tf.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> APPLYING EPSILON-GREEDY POLICY</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy for action selection\n",
    "def select_action(state, model, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.choice(4)  # Random action choice\n",
    "    else:\n",
    "        return np.argmax(model.predict(np.array([state])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> INITIALIZING LANE ID'S</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lane IDs used in the SUMO simulation\n",
    "lanes = [\n",
    "    \":2184943151_0_0\",\n",
    "    \":2586216133_0_0\",\n",
    "    \":293196396_0_0\",\n",
    "    \":293196396_1_0\",\n",
    "    \":293196396_2_0\",\n",
    "    \":293196396_3_0\",\n",
    "    \":293196396_16_0\",\n",
    "    \":293196396_17_0\",\n",
    "    \":293196396_4_0\",\n",
    "    \":293196396_5_0\",\n",
    "    \":293196396_6_0\",\n",
    "    \":293196396_7_0\",\n",
    "    \":293196396_18_0\",\n",
    "    \":293196396_19_0\",\n",
    "    \":293196396_8_0\",\n",
    "    \":293196396_9_0\",\n",
    "    \":293196396_10_0\",\n",
    "    \":293196396_11_0\",\n",
    "    \":293196396_20_0\",\n",
    "    \":293196396_21_0\",\n",
    "    \":293196396_12_0\",\n",
    "    \":293196396_13_0\",\n",
    "    \":293196396_14_0\",\n",
    "    \":293196396_15_0\",\n",
    "    \":293196396_22_0\",\n",
    "    \":293196396_23_0\",\n",
    "    \":295532041_0_0\",\n",
    "    \":5192644484_0_0\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> STATE FUNCTION</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the current state from the simulation\n",
    "def get_state():\n",
    "    queue_lengths = [traci.lane.getLastStepHaltingNumber(lane) for lane in lanes]\n",
    "    signal_state = traci.trafficlight.getRedYellowGreenState(\"293196396\") # Traffic light ID\n",
    "    signal_state_vector = [1 if state == 'G' else 0 for state in signal_state]\n",
    "    elapsed_time = traci.trafficlight.getPhaseDuration(\"293196396\") - traci.trafficlight.getNextSwitch(\"293196396\")\n",
    "    state = queue_lengths + signal_state_vector + [elapsed_time]\n",
    "    return np.random.rand(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> REWARD FUNCTION</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the reward based on the current traffic situation\n",
    "def get_reward():\n",
    "    waiting_time = sum(traci.lane.getWaitingTime(lane) for lane in traci.lane.getIDList())\n",
    "    reward = -waiting_time # Negative reward for longer waiting times\n",
    "    return np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> REPLAY BUFFER</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to handle storing and sampling experiences for training\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> TRAINING FUNCTION</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defining a single training step using a mini-batch from the replay buffer\n",
    "def train_model(model, target_model, replay_buffer, batch_size, gamma):\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    # Convert batch data to numpy arrays for processing\n",
    "    states = np.array(states)\n",
    "    next_states = np.array(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    actions = np.array(actions)\n",
    "    dones = np.array(dones)\n",
    "\n",
    "    # Calculate the next Q-values using the target model\n",
    "    next_q_values = target_model.predict(next_states)\n",
    "    max_next_q_values = np.max(next_q_values, axis=1)\n",
    "\n",
    "    # Compute the target Q-values using the Bellman equation\n",
    "    target_q_values = rewards + (gamma * max_next_q_values * (1 - dones))\n",
    "\n",
    "    # Update the Q-values for the actions taken\n",
    "    q_values = model.predict(states)\n",
    "    q_values[range(batch_size), actions] = target_q_values\n",
    "\n",
    "    # Perform a gradient descent step to update the model\n",
    "    model.fit(states, q_values, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> HYPER PARAMETERS</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial hyperparameters for the training process\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_min = 0.01  # Minimum exploration rate\n",
    "epsilon_decay = 0.95  # Decay rate for exploration-exploitation balance\n",
    "rewards_during_dqn = []  # To store rewards for each episode for analysis\n",
    "num_episodes = 10  # Total number of episodes for training\n",
    "max_steps_per_episode = 1000  # Maximum steps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> SIMULATION</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the replay buffer and training variables\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "batch_size = 32  # Number of experiences to sample from buffer\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "target_model = tf.keras.models.clone_model(model) # Clone model for stable Q-value estimation\n",
    "\n",
    "# Start the SUMO simulation using TraCI\n",
    "traci.start(sumoCmd)\n",
    "while not traci.simulation.getMinExpectedNumber() <= 0:\n",
    "\n",
    "\n",
    "\n",
    "# Initialize metrics for analysis\n",
    "    average_queue_lengths = []  # To monitor traffic queue lengths\n",
    "    actions_taken = []  # To analyze the distribution of actions taken\n",
    "    step_rewards = [[] for _ in range(num_episodes)]  # Nested list to store rewards per step per episode\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        traci.load([\"-c\", sumoConfigPath, \"--start\", \"--quit-on-end\"])  # Restart the scenario for each episode\n",
    "        state = get_state()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = select_action(state, model, epsilon) # Select an action using the policy\n",
    "            \n",
    "            traci.simulationStep() # Advance the simulation\n",
    "            \n",
    "            next_state = get_state()\n",
    "            reward = get_reward()\n",
    "            done = step == max_steps_per_episode - 1  # Check if episode should end\n",
    "            replay_buffer.add(state, action, reward, next_state, done) # Store experience in buffer\n",
    "            total_reward += reward\n",
    "            step_rewards[episode].append(reward) # Log rewards for analysis\n",
    "\n",
    "            # Train the model every 'batch_size' steps\n",
    "            if len(replay_buffer.buffer) >= batch_size:\n",
    "                train_model(model, target_model, replay_buffer, batch_size, gamma)\n",
    "\n",
    "            state = next_state  # Transition to the next state\n",
    "            actions_taken.append(action)  # Log the action taken\n",
    "\n",
    "            # Calculate average queue length for this step and log it\n",
    "            current_queue_lengths = [traci.lane.getLastStepHaltingNumber(lane) for lane in lanes]\n",
    "            average_queue_length = sum(current_queue_lengths) / len(lanes)\n",
    "            average_queue_lengths.append(average_queue_length)\n",
    "\n",
    "        # Periodically update the target model to stabilize learning\n",
    "        if episode % 10 == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        # Update exploration rate\n",
    "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "        rewards_during_dqn.append(total_reward) # Store total reward for this episode\n",
    "\n",
    "traci.close() # Close the TraCI connection to SUMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> REWARDS RANGE</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the range of total rewards and rewards per step for analysis\n",
    "print(\"Total rewards range:\", min(rewards_during_dqn), max(rewards_during_dqn))\n",
    "print(\"Step rewards range:\", min(min(step_rewards)), max(max(step_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> PLOT - REWARDS OVER TIME FOR EACH EPISODE</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine layout for subplot grid based on the number of episodes\n",
    "cols = 5  \n",
    "rows = (num_episodes + cols - 1) // cols \n",
    "episodes = max(0, num_episodes - 10)  # Start plotting from the last 10 episodes\n",
    "\n",
    "# Create a subplot for each episode to plot rewards over time\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, rows * 4), constrained_layout=True)\n",
    "fig.suptitle('Rewards Over Time for Each Episode')\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    # Loop over each episode and configure subplot for reward plot\n",
    "    row = ep // cols\n",
    "    col = ep % cols\n",
    "    ax = axs[row, col] if rows > 1 else axs[col]\n",
    "\n",
    "    # Plot the rewards for the episode\n",
    "    ax.plot(step_rewards[ep])\n",
    "    ax.set_title(f'Episode {ep+1}')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.label_outer()  # Hide labels on inner plots for clarity\n",
    "\n",
    "# Hide unused subplots if there are fewer episodes than slots in the grid\n",
    "for i in range(episodes, rows * cols):\n",
    "    fig.delaxes(axs.flatten()[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> PLOT - TOTAL REWARD OVER EPISODES</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total reward progression over multiple episodes\n",
    "plt.figure()\n",
    "plt.plot(range(len(rewards_during_dqn)), rewards_during_dqn)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward Over Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <B><CENTER> PIE CHART - DISTRIBUTION OF ACTIONS TAKEN</CENTER></B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of actions taken throughout the simulation\n",
    "action_counts = Counter(actions_taken)\n",
    "actions, frequencies = zip(*sorted(action_counts.items()))\n",
    "\n",
    "# Visualize the action distribution in a pie chart\n",
    "plt.figure(figsize=(8, 8)) \n",
    "plt.pie(frequencies, labels=actions, autopct='%1.2f%%', startangle=140)\n",
    "plt.title('Distribution of Actions Taken')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
